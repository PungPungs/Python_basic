{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73402b7a",
   "metadata": {},
   "source": [
    "### 활성화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568dd2b0",
   "metadata": {},
   "source": [
    "#### softmax\n",
    "$$\n",
    "softmax(x_i) = \\frac{e^{x_i}}{\\sum_{k = 1}^{k} e^{z_i}}\n",
    "$$\n",
    "- 기존 softmax 의 함수는 이렇게 정의된다. 하지만 컴퓨터 상의 softmax는 e (지수함수)의 값이 커지면 오버플로우가 발생하기 때문에 보통은 제일 큰 값을 빼서 계산을 하는 것이다.\n",
    "- 질문 : 그러면 선생님, 기존 값을 뺀 함수는 값이 다르지 않을까요?\n",
    "- 대답 : 함수 위 아래에 똑같은 값을 대입하여 나누거나 곱하는 것은 결과가 항상 같기 때문에 값은 똑같이 나온단다.\n",
    "\n",
    "#### softmax(Activate)\n",
    "$$\n",
    "\\sigma(x_i) = \\frac{e^{(x_i - max(x_i))}}{\\sum_{k=1}^{k} e^{(x_i - max(x))}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / ex.sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde73d53",
   "metadata": {},
   "source": [
    "#### sigmoid\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "- 이전에는 역전파를 구하기 위해 미분을 위한 sigmoid 함수를 사용했지만 0에 가까워질수록 기울기가 사라지는 gradient vanishing 이 발생하였다. 이를 해결하기 위해 ReLU, softmax 등 다양한 활성화 함수가 등장하게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc546b1f",
   "metadata": {},
   "source": [
    "### 손실함수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75933fb2",
   "metadata": {},
   "source": [
    "#### cross entropic error\n",
    "$$\n",
    "E = -\\sum_{k}^{t_k} \\log{(y_k)}\n",
    "$$\n",
    "\n",
    "- 오차를 구하기 위한 loss 함수이다. 이 손실함수도 로그의 특성상 0이 들어가면 무한대로 빠져 값을 구하지 못하기 때문에 프로그램 구현에서는 가장 작은 값을 빼준다.\n",
    "- 위 공식에서 $t_k$는 정답 레이블, $y_k$는 예측으로 얻은 값이다.\n",
    "\n",
    "```python\n",
    "def cross_entropic_error(x,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(x + delta))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154e610",
   "metadata": {},
   "source": [
    "### 수치미분(Numerical Differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59420f6d",
   "metadata": {},
   "source": [
    "#### 수치미분의 종류\n",
    "1. 전방 미분(forward difference)\n",
    "$$\n",
    "f(x) = \\frac{(f+h) - f(x)}{h}\n",
    "$$\n",
    "2. 후방 미분(backward difference)\n",
    "$$\n",
    "f(x) = \\frac{(f) - f(x-h)}{h}\n",
    "$$\n",
    "3. 중앙 미분(centeral difference)\n",
    "$$\n",
    "f(x) = \\frac{(f+h) - f(x-h)}{2h}\n",
    "$$\n",
    "\n",
    "- 1 ~ 3 번 식 중 3번으로 구하는 차분이 제일 정확하기 떄문에 주로 3번을 쓴다. 그리고 미분 또한 0으로 수렴할 수 없기 떄문에 가장 작은 값을 더해주는데 파이썬 특성 상 일정 소수점 이하 부터는 반올림을 하기 때문에 주로 1e-4 $10^{-4}$을 쓴다. 프로그램상으로는 중앙 미분만 구현하겠다.\n",
    "- 수치 미분은 정확한 미분값이 아니라 컴퓨터로 구현한 근삿값(approximation)으로 계산하는 걸 의미한다.\n",
    "- 차분은 수치 미분에 사용된 방정식을 정의한다.\n",
    "\n",
    "```python\n",
    "def centeral_differential(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e94b30",
   "metadata": {},
   "source": [
    "#### 편미분(partial derivative)\n",
    "- 편미분이란 어떤 함수의 변수의 영향력을 알고 싶을 때 사용하는 방식이다. 변수 하나를 고정시켜두고 나머지는 상수로 보고 계산하는 방식이다. 편미분도 수치미분을 이용하여 계산한다. 공식은 미분이랑 동일하지만 고정 변수에 대해서만 미분한다는 점이다.\n",
    "$$\n",
    "\\frac{\\nabla f}{\\nabla x}\n",
    "$$\n",
    "```python\n",
    "def  partial_derivative(f,x):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43b206",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5fded",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x - max(x)) / np.sum(x - np.max(x))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
